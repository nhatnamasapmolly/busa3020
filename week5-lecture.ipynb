{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Week 5 Data Preprocessing: Building Good Training Datasets   \n\n## Unit Convenor & Lecturer  \n\nElias Maroun\n\n[elias.maroun@mq.edu.au](mailto:elias.maroun@mq.edu.au)\n\n## References    \n1. Python Machine Learning 3rd Edition by Raschka & Mirjalili - Chapter 4\n2. Various open-source material\n\n## Learning Objectives   \n\n- Issues with missing data\n- Types of missingness and their consequences\n- Removing and Imputing Missing Values from the Dataset\n- Getting Categorical Data into Shape for Use with Machine Learning Algorithms\n- Selecting Relevant Features for Model Construction\n- Feature Importance\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "--- \n---\n# Missing Data  \n\n\nIf the data is inaccurate, missing or irrelevant we will not be able to produce good predictions (no matter how good and sophisticated our forecasting algorithm is)\n\n- **Quality** and the **amount of useful information** are key determinants of how well a machine learning algorithm can learn\n- We must examine and preprocess data before we use it to train a machine learning model\n\n\n\n",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Dealing with Missing Data   \n\n- Data is often missing\n    - Errors in data collection \n    - Certain data doesn't exist \n    - Surveys are incorrectly answered, e.g. income often understated\n    - Etc\n\n### Missingness of Data and Its Consequences\n\n- Understanding the **nature of missing data** is crucial in determining how to handle it effectively\n- The pattern and mechanism of missing data can significantly impact the outcome of the analysis \n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Types of Missingness\n\n1. **Missing Completely at Random (MCAR)**\n- Data is missing entirely at random\n- No relationship between the missing data and any observed or unobserved variables\n- The likelihood of a data point being missing is the same across all observations\n- Example: A survey respondent accidentally skips a question\n- Consequences: If data is MCAR, any analysis performed on the data remains unbiased, even if missing data is simply ignored or removed",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "2. **Missing at Random (MAR)**\n- The missingness is related to some of the observed data, but not the missing data itself\n- The probability of data being missing is dependent on other observed variables, but not the missing variable itself\n- Example: In a medical study, younger patients may be less likely to report their weight, leading to missing data that depends on age but not on weight directly\n- Consequences: Data that is MAR can lead to biased results if not handled properly\n    - Imputation methods that take other observed variables into account, like multiple imputation or regression-based methods may be appropriate here",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "3. **Missing Not at Random (MNAR)**\n- The missingness is related to the value of the missing data itself  \n- The reason data is missing is due to the actual value of the data that is not observed\n- Example: In a study on income, people with very high or very low incomes might be less likely to disclose their income\n    - This makes the missingness directly related to income\n- Consequences: MNAR is the most challenging type of missing data to handle\n    - This type of missingness introduces significant bias that simple imputation methods can't address\n    - Advanced techniques or domain-specific models are often required,\n    - The missing data mechanism needs to be explicitly modeled\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Missing Data in Python \n\n- Common placeholders for missing observations in databases are `NaN` - 'not a number' or `NULL`\n- If we try to train an algorithm using such dataframes we will often get an error\n- `scikit-learn` was originally developed to work with `NumPy` arrays\n    - Clean data in `pandas` and then export into `numpy` using `df.values`\n    - Newer versions of some `scikit-learn` libraries also work with `pandas` dataframes\n\n<hr style=\"width:25%;margin-left:0;\">   \n\n### Identifying Missing Values in Tabular Data \n\n- When dealing with missing data it is easiest to use `pandas`\n    - `isnull` method returns a `DataFrame` with Boolean values to indicate whether a cell contains a numeric value (`False`) or if data is missing (`True`)\n    - we can also `.sum()` after `isnull` which treats False = 0, and True = 1 -> get a number of missing values for each column or row\n    - `pandas` method `.info()` also provides a count of Non-Null Values for each column\n    \n- Lets create some missing data in a `pandas` DataFrame\n\n```\nimport pandas as pd\nimport numpy as np\nfrom io import StringIO # allows us to read from a string as if we are reading from a file\n\ncsv_data = 'A, B, C, D \\n 1.0, 2.0, 3.0, 4.0 \\n 5.0, 6.0,, 8.0 \\n 10.0, 11.0, 12.0,'  # note \\n inserts a line break (new line)\n# print(csv_data, '\\n', type(csv_data))\n\ndf = pd.read_csv(StringIO(csv_data))\ndf\n\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Now we can detect the missing data\n```\ndf.info()\n\ndf.isnull()\n\nprint('Missing Values: \\n',df.isnull().sum().sort_values())\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Columns C and D have one missing value each. No other columns have missing values.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n<hr style=\"width:25%;margin-left:0;\"> \n\n### Eliminating Training Examples with Missing Values \n\n- The easiest but probably **NOT** the best way to deal with missing data is to remove missing observations (either rows or columns)\n    - This would be ok if our dataset is large and the data is missing completely at random (MCAR)\n    - If the dataset is of moderate or small size then it would lead to a sinficant loss of information",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- In `pandas` we can use `dropna()` method\n    - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n    - `dropna(axis=1)` - remove missing features (columns)\n    - `dropna(axis=0)` - remove missing training examples (rows)\n    - Most `pandas` methods have `inplace` parameter \n        - If True, do operation inplace (on our DataFrame) and return None, i.e. change the DataFrame\n\n```\nprint(df)\nprint(df.dropna(axis=1)) # drop columns with missing values\nprint(2*'\\n')\nprint(df)\n\n\nprint(df)\nprint(df.dropna(axis=0))  # drop rows with missing values\nprint(2*'\\n')\nprint(df)\n\n\nprint(df)\nprint(df.dropna(how='all')) # drops rows where all columns are NaN\nprint(2*'\\n')\nprint(df)\n\n\nprint(df)\nprint(df.dropna(axis = 0, inplace=True)) # drop rows with missing values and with saves the changes made to df\nprint(2*'\\n')\nprint(df)\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n## Imputing Missing Values \n\n- *Deleting missing values is not a good option when we have a small dataset or the data is not missing completely at random (MCAR)*\n    - Often we have limited amounts of data and need to preserve as much data as we can\n    - There are many imputation methods such as for example:\n        -  KNN imputation\n        -  Predict missing values based on other features, e.g. regression models or decision trees\n        -  Forward/backward fill (for time series data)\n        -  Mean/Median/Mode Imputation\n        -  Etc.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- In **this unit** we are going to consider Mean/Median/Mode Imputation (Interpolation) to fill in missing values\n    - This simple method works well in practice for predictive purposes\n    - If we are also concerned with model interpretation then more elaborate imputation methods are required\n        - Interpretation vs Prediction: if I change x by a certain amount what happens to y vs. for a given value of x what is the prediction of y\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Definitions \n- **Mean** (Average) = $\\frac{1}{N}\\sum x_i$\n- **Median** is the value that separates a dataset into two equal halves\n    - It is the middle value when the data points are arranged in ascending or descending order\n    - If there is an odd number of data points, the median is the middle value\n    - If there is an even number of data points, the median is the average of the two middle values\n    - Example: In the dataset 3, 5, 7, 9, 11 the median is 7 (since it's the middle value)\n- **Mode** is the most frequently occurring value(s) in a dataset\n    - Uni-modal: A dataset can have a single mode if one value occurs more frequently than others.\n    - Bi-modal (Multi-model): A dataset is bimodal if two different values occur with the same highest frequency\n    - Example: In the dataset 2, 4, 4, 6, 8, 8, 8, 10 the mode is 8 (because it appears three times, more often than any other value)\n    - For certain types of data (e.g. nominal, see below) mode is particarly useful since it is impossible to compute the mean or median",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Types of Data in Predictive Analytics\n\n- Which value (mean/median/mode) we for imputation depends on the type of data that is missing\n- Variables can be categorised into different types based on their characteristics and how they are represented\n- Here are some main types of data in analytics\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "1. **Categorical Data**\n- Data that represent categories or groups\n- The values are discrete and often qualitative\n    - Nominal: Categories that have no intrinsic order\n        - Examples include gender (male, female), color (red, blue, green)\n    - Ordinal: Categories that have a clear, ordered relationship\n        - Examples include rankings (first, second, third), education levels (high school, bachelor's, master's)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "2. **Numerical Data**\n- Data that represent numbers and can be used in mathematical operations\n    - Continuous: Data that can take any value within a range\n        - Data that is measured\n        - Examples include height, weight, temperature\n    - Discrete: Data that can take specific, distinct values, often integers\n        - Usually data that is counted\n        - Examples: number of books in a bookstore, number of students in a class, etc.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "3. **Text Data**\n    - Data in the form of natural language, such as sentences or words\n    - Examples: Tweets, reviews, articles",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "4. **Image Data**\n    - Data represented in the form of images, often as pixel values in matrices.\n    - Examples: Photographs, medical scans, satellite images",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "5. **Audio**, **Video**, etc.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n## Mean/Median/Mode Interpolation    \n\nIn our business applications we typically encounter situtations where *categorical* and *numerical* data is missing   \n- Mean/Median/Mode Interpolation: Replace missing values with the mean, median, or mode of the column\n- However we have to be careful about the type of data we have\n    - **Numeric data** -> we typically use mean or median (if concerned about outliers or when the distribution is not symmetrical)    \n    - **Categorical data** -> use median or mode (the only value that can be used with nominal data that can't be ordered)   \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\nThere are a number of libraries that can help deal with missing data\n1. `SimpleImputer` class from `scikit-learn` is quite useful \n    - [https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n    - Can do `mean`, `median`, `most frequent` and `constant` imputation methods by setting `strategy` parameter\n    - `most_frequent` method is useful for categorical feature values, e.g. colour names: red, green blue \n2. `pandas` has a built in `fillna` method\n    - [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)\n\n\n### Imputing with `scikit-learn`\n\nLets interpolate missing values in our dataframe using **mean imputations** using scikit-learn\n\n```\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\ndf = pd.read_csv(StringIO(csv_data)) # --------- read in data with missing observations again ----------\ndf\n\n# ---------------- sklearn method -------------------------\n\nimr = SimpleImputer(missing_values = np.nan, strategy='mean') # use mean imputation\nimr = imr.fit(df)  # .values is used to export pandas dataframe into numpy array\n\nimputed_data = imr.transform(df)\nimputed_data # notice that the data is transferred back into a NumPy array\n\n\n\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:25%;margin-left:0;\"> \n\n### Scikit-learn in more depth {-}\n\n- `SimpleImputer` belongs to the **transformer** class in scikit-learn, which are used for data transformation\n- Transformers have two main methods\n    - `fit` - used to learn parameters, e.g. column means, from training data\n    - `transform` - used learned parameters to transform data\n- Any data that is to be transformed must have the same number of features as the dataset that was used to fit the transformer\n\n<img src=\"images/image1.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/>\n\n\n\n<!-- ![](images/image2.jpg) -->",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Imputing with `Pandas`\n\n```\n\n# ---------------- pandas method -------------------------\n\ndf = pd.read_csv(StringIO(csv_data))  # reset df with missing values #  --------- read in data with missing observations again ----------\ndf\n\nprint(df.mean(axis=0)) \nprint(df.fillna(df.mean(axis = 0), inplace=True))  #fill NaN with column mean values\ndf\n```\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n---\n\n# Getting Categorical Data into Shape for Use with Machine Learning Algorithms {-}\n\n- So far we have mainly used **numerical** data as **features** to build predictive models\n- When using **categorical** features\n    - Need to make sure that such features are encoded correctly \n    - **Ordinal Features** \n        - In order to use classifiers on ordinal data properly we need to convert such variables into **integers**\n        - Example: consider shirt sizes: XL > L > M > S  then we would could encode these values as follows: XL = 4, L = 3, M = 2, S = 1   \n    - **Nominal Features**:\n        - No ordering possible -> we need to encode values using **one-hot encoding** (**dummy variables**)\n        - Example: colour: {green, blue, red} see below for encoding ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\nConsider the following dataset containing:\n- a nominal feature (colour)   \n- an ordinal feature (size)   \n- a numerical feature (price)  \n\n\n    \n```\nimport pandas as pd\n\ndf = pd.DataFrame([\n    ['yellow', 'S', 8.2, 'class2'],\n    ['green', 'M', 10.1, 'class2'],\n    ['red', 'L', 13.5, 'class1'],\n    ['blue', 'XL', 15.3, 'class2']])\n\ndf.columns = ['colour', 'size', 'price', 'classlabel']\ndf\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Mapping Ordinal Features   \n\nTo map ordinal features which are currently encoded as **string** python types into **integers** we need to take care of the ordering of the labels   \n\n- Often we need to do this manually  \n- Create a new column whose values reflect ordinal feature labels mapped into integers  \n- The easiest way is to create a python **dictionary** and map it into the original column\n    - A Python dictionary is a collection of key-value pairs where each key is unique and is used to store and access data\n    - They are defined using curly braces {} with keys and values separated by colons\n- Here's a short example of creating and accessing a dictionary:\n\n```\nmy_dict = {'name': 'John', 'age': 30}\n\nprint(my_dict['name'])  # Accesses the value associated with key 'name'\n\nmy_dict['age'] = 31  # Updates the value of the 'age' key\n\nmy_dict\n```\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "\n- We will use python dictionaries together with the `map` in `pandas`\n    - [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html)   \n- Used for substituting each value in a Series with another value, that may be derived from a function, a dict or a Series.\n- Lets consider the example of shirt sizes\n\n```\ndf\n\nsize_mapping = {'XL':4, 'L':3, 'M':2, 'S':1}\nprint(size_mapping, type(size_mapping))\n\ndf['size2'] = df['size'].map(size_mapping)\n\ndf\n\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n### One-Hot Encoding of Nominal Features {-}  \n\n- We must be careful not to encode **nominal features** using integer values that represent an ordering    \n    - E.g. red = 1, blue = 2, green = 3\n    - This would confuse *linear* classifiers as it will assume that somehow green > blue > red\n    - Consider a linear regression using with a positive slope\n        - The prediction for blue would always be between the predictions for green and red    \n\n\n<img src=\"images/colours.jpg\" alt=\"Drawing\" style=\"width: 450px;\"/>\n   ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "One-Hot Encoding and Dummy Variables are both techniques used to convert categorical variables into a numerical format that can be used in machine learning models \n- **One-Hot Encoding**\n    - One-hot encoding transforms each category in a categorical variable into a new binary variable (column) with values of 0 or 1\n    - Each original category gets its own binary column\n    - If a categorical variable has $n$ unique categories, one-hot encoding will create $n$ new columns\n    - For each observation/row, the column corresponding to the category will be set to 1, and all other columns will be set to 0\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- There are a number of ways to do this\n    - `OneHotEncoder` scikit-learn libarary\n    - `get_dummies` method from `pandas`   \n        - [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)\n\n    \n```\ndf\n\none_hot = pd.get_dummies(df[['colour']], dtype = int)\nprint(one_hot)\n\ndf2 = df.join(one_hot)\ndf2\n\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "- **Dummy Variables**\n    - Dummy variables are similar to one-hot encoding but typically drop one category to avoid multicollinearity\n        - Especially in the context of linear models\n    - The dropped category can be inferred from the remaining categories\n    - If a categorical variable has $n$ unique categories, dummy variable encoding will create $n−1$ new columns\n    - The dropped category is usually the one that acts as the baseline against which the other categories are compared\n \n```\none_hot = pd.get_dummies(df[['colour']], dtype = int, drop_first = True)\nprint(one_hot)\n\ndf3 = df.join(one_hot)\ndf3\n\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n### Encoding Class Labels\n\nAlthough most scikit-learn estimators convert class labels (target variable) to integers internally it is best to provide class labels as **integer arrays** to avoid any technical glitches\n- **Class labels are not necessarily ordinal variables**\n    - E.g. trying to predict sales channels for marketing purposes: Online, In-Store, Mobile App, Call Center, Direct Mail\n    - Doesn't matter which integer we assign to a particular label\n    - It is easiest to enumerate class labels starting at 0\n\nWe can use `LabelEncoder` class from scikit-learn for this  \n- Use `fit_transform` method to encode labels  \n- Use `inverse_transform` to transform integer labels back to their original string representations\n    \n```\nfrom sklearn.preprocessing import LabelEncoder\n\ndf3\n\nclass_le = LabelEncoder()\ndf3['y'] = class_le.fit_transform(df3['classlabel'].values)\ndf3\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Creating $X$ and $y$ arrays\n\n```\ny = df3['y']\ny\n\n\nX = df3[['price', 'size2', 'colour_green', 'colour_red', 'colour_yellow']]\nX\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---  \n# Feature Scaling (Again) {-}\n\nApart from **decision trees** and **random forests** most machine learning algorithms require feature scaling\n- Transforming features to the same scale ensures that weights are better optimised    \n\nConsider a random variable $X\\sim(\\mu,\\sigma)$, i.e. $X$ has the mean value of $\\mu$ and standard deviation of $\\sigma$\n\n- Typically there are two approaches for feature scaling  \n1. **Standardization** which we have seen before: if  $X_{stand.} =\\frac{X-\\mu}{\\sigma}$\n    - $X_{stand.}$ will have the mean value of 0 and standard deviation of 1, i.e.  $X_{stand.}\\sim(0,1)$\n    - use `StandardScaler` class from scikit-learn\n3. **Normalization** scales features to a range of $[0,1]$ interval\n    - E.g. **min-max scaling** $X_{norm}=\\frac{X-X_{min}}{X_{max} - X_{min}}$\n    - use `MinMaxScalar` class",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\nWhich one is more appropriate?  \n- It really depends on data properties and should be evaluated on a case-by-case basis  \n- Standardization can be more appropriate for optimization algorithms such as gradient descent\n- In some cases normalization is more easily interpreted\n- Can always try both and see which method produces better forecasts  \n    \nLets create some data and see how standardization and normalization compare\n\n```\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame(range(0, 6), columns=['X'])\ndf\n\nstandard_scaler = StandardScaler()\ndf['X_standardized'] = standard_scaler.fit_transform(df['X'].values.reshape(-1,1))\ndf\n\nnorm_scaler = MinMaxScaler()\ndf['X_normalized'] = norm_scaler.fit_transform(df['X'].values.reshape(-1,1))\n\ndf\ndf.plot()\n\nplt.axhline(y=-1, color='gray', linestyle='--', label='y=-1')\nplt.axhline(y=0, color='gray', linestyle='--', label='y=0')\nplt.axhline(y=1, color='gray', linestyle='--', label='y=1')\nplt.show()\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n---\n\n# Selecting Meaningful Features & Overfitting (again) {-}   \n\n- Models typically perform better on training datasets than on test datasets. \n    - Why does this happen?  \n- Model is overfitted when\n    - The model fits to the data in the training dataset too closely, including random variations in the training dataset\n    - The model then tries to predict such random variations in the test dataset but because the variations were random in the first place they don't repeat in the test data  \n    - The possibility of overfitting arises when models are too complex  \n    - Overfitted models are said to have high variance, because the predictions from overfitted models vary a lot  \n    - We also say that the model does not generalize well to new data\n    - For a more complete discussion see Week 3 lecture notes\n- However, poor performance on test data can also happen when the model has not been optimized fully, or the hyperparameters are not selected appropriately\n\n\nWhen is a model too complex?    \n- In practice this can mean that the model has too many parameters   \n    - We are considering too many features/explanatory variables   \n    \n\nPossible solutions for overfitting    \n- Collect more data and train the model on larger datasets  \n- Choose a simpler model with less parameters  \n- Introduce a penalty for complexity via regularization  \n- Reduce the dimension of the data  \n\nNext we will discuss \n\n- Regularization (again)\n- Dimensionality reduction via feature selection\n\nBoth of these techniques will lead to fewer parameters to be fitted to the data and hence reduce the amount of overfitting.\n\n    \n    ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\"> \n\n### L1 and L2 Regularisation as Penalties Against Model Complexity {-}   \n\nIn Week 3 we introduced L2 regularisation as a method of reducing model complexity\n\n- Large parameter values are penalised in the cost function\n    - This results in **smaller values** of the optimised weights\n        - Example: before regularisation $w_1=1.3$, after regularisation $w_1 = 0.7$ \n    - Smaller weights mean that predictor variables have a smaller impact on the forecasts \n    - Weights which were already small before regularization will become zero, eliminating the influence of their corresponding predictors",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Typically, we use L1 and L2 regularisations:\n\n- L2 uses the values of squared parameters (this designates 2 in L2)\n    - L2: $||w||_2^2=\\sum_{j=1}^mw_j^2=w_1^2+w_2^2+\\dots+w_m^2$\n- L1 regularisation employes absolute values\n    - L1: $||w||_1=\\sum_{j=1}^m|w_j|=|w_1|+|w_2|+\\dots+|w_m|$",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Comparison of L1 and L2 regularizations**\n\n- *Robustness*: L1 is better than L2\n    - Robustness is defined as resistance to outliers in a dataset. \n    - The more able a model is to ignore extreme values in the data, the more robust it is. \n    - L1 norm is **more robust** than the L2 norm\n    - L2 norm squares values, so it increases the cost of outliers exponentially and hence tries to make large errors associated with outliers smaller\n    - L1 norm only takes the absolute value, so it considers them linearly\n- *Number of Solutions*: L2 has one solution which is better than L1 that has many solutions\n    - Because L2 is Euclidean distance, there is always one right answer as to how to get between two points fastest.\n    - L1 is taxicab distance and there are as many solutions. There could be another (mirror image) path of the same length as the blue path.\n- *Computational difficulty*: L2 is easier to optimise than L1\n    - L2 has a closed form solution because it's using the square function which can be differentiated \n    - L1 does not have a closed form solution because it is a non-differenciable piecewise function, as it involves an absolute value. \n    - For this reason, L1 is computationally more expensive as it mostly rely on approximations (in the lasso case, coordinate descent).\n- *Sparsity*: L1 is better for feature selection than L2\n    - Sparse matrices or sparse arrays are matrices in which most of the elements are zero. \n    - By contrast, if most of the elements are nonzero, then the matrix is considered dense. \n    - While both L1 and L2 penalties shrink coefficients, L1 tends to shrink coefficients to zero whereas L2 tends to shrink coefficients evenly. \n    - L1 is therefore useful for feature selection, as we can drop any variables associated with coefficients that are estimated to be zero. \n    - This makes a model robust to potentially irrelevant features in the dataset.\n    - L2, on the other hand, is useful when we have multicollinearity.\n    ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### Impact of Regularization on Logistic Regression Weights {-}\n\nWe'll investigate the impact of regularisation and $C$ (inverse of regularization strength) on Logistic Regression weights \n\n- Import the wine dataset from [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)   \n\n    - 178 wine examples\n    - 13 features describing their different chemical properties\n \n\n```\n# import pandas as pd\n# import numpy as np\n\n# df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data')\n\n\n# df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', \n#                    'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n#                    'Proline']\n\n# df_wine.to_excel('data/wine.xlsx', index=False)\n\ndf_wine = pd.read_excel('data/wine.xlsx')\nprint('Class labels', np.unique(df_wine['Class label']))\ndf_wine.head()\n\n\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n- Use `train_test_split` libarary to split the data into 70% train and 30% test datasets. \n- Stratify according to Class label\n    \n   \n    \n```\nfrom sklearn.model_selection import train_test_split\n\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n    \n- Standardize training and test datasets\n\n\n```\nfrom sklearn.preprocessing import StandardScaler\n\nstdsc = StandardScaler()\n\nX_train_std = stdsc.fit_transform(X_train)\nX_test_std = stdsc.transform(X_test)\n```\n    ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "* Normalize training and test datasets (**Normalization** scales features to a range of [0,1] interval)\n\n```\nfrom sklearn.preprocessing import MinMaxScaler\n\nnorsc = MinMaxScaler()\n\nX_train_nor = norsc.fit_transform(X_train)\nX_test_nor = norsc.transform(X_test)\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n- Fit `LogisticRegression` to the training data using L1 penalty (`penalty='l1'`, `C=1.0`, `solver='liblinear'`, `multi_class='ovr'`). \n- Compute accuracy for both training and test datasets.\n\n\n```\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(penalty='l1', C=0.1, solver='liblinear', multi_class='ovr')\n# Note that C=1.0 is the default. You can increase\n# or decrease it to make the regulariztion effect\n# stronger or weaker, respectively.\n\nlr.fit(X_train_std, y_train)\n\nprint('Training accuracy:', lr.score(X_train_std, y_train))\nprint('Test accuracy:', lr.score(X_test_std, y_test)) \n\n\n# ------- using normalized dataset ---------------\n\nlr_nor = LogisticRegression(penalty='l1', C=0.1, solver='liblinear', multi_class='ovr')\n\n# Note that C=1.0 is the default. You can increase\n# or decrease it to make the regulariztion effect\n# stronger or weaker, respectively.\n\nlr_nor.fit(X_train_nor, y_train)\n\nprint('Training accuracy (normalized):', lr_nor.score(X_train_nor, y_train))\nprint('Testing accuracy (normalized):', lr_nor.score(X_test_nor, y_test))\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n- Print intercepts and weight coefficients of the fitted model\n\n```\nprint('itercept:',lr.intercept_,'\\n')\nprint('coef:', lr.coef_,'\\n')\nprint(\"coef's shape:\", lr.coef_.shape)    \nprint(2*'\\n')\nprint('coef not zero:',lr.coef_[lr.coef_!=0],'\\n')\nprint(\"coef not zero's shape:\", lr.coef_[lr.coef_!=0].shape)\n```\n    ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### L1 vs L2 Regularization {-}\n\n\n```\n# L1 Regularization VS L2 Regularization\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfig = plt.figure(figsize=(8,12))\ngs = gridspec.GridSpec(2,1)\n\n# standardized data\nax = fig.add_subplot(gs[0])\n\ncolors = ['blue', 'green', 'red', 'cyan', \n          'magenta', 'yellow', 'black', \n          'pink', 'lightgreen', 'lightblue', \n          'gray', 'indigo', 'orange']\n\nweights, params = [], []\nfor c in range(-4, 6):\n    lr = LogisticRegression(penalty='l1', C=10.**c, solver='liblinear', \n                            multi_class='ovr', random_state=0)\n    print(10.**c)\n    lr.fit(X_train_std, y_train)\n    weights.append(lr.coef_[1])\n    params.append(10**c)\n\nweights = np.array(weights)\n\nfor column, color in zip(range(weights.shape[1]), colors):\n    ax.plot(params, weights[:, column],\n             label=df_wine.columns[column + 1],\n             color=color)\n\nplt.title(\"Fitted coefficients for different regularisation strengths (L1)\",\n         fontdict={'weight':'normal','size': 14})\nplt.axhline(0, color='black', linestyle='--', linewidth=3)\nplt.axvline(x=0.007, color='gray', linestyle='--', label='C=0.007')\nplt.axvline(x=0.1, color='gray', linestyle=':', label='C=0.1')\n\nplt.xlim([10**(-5), 10**5])\nplt.ylabel('weight coefficient')\nplt.xlabel('C')\nplt.xscale('log')\nplt.legend(loc='upper left')\nax.legend(loc='upper center', bbox_to_anchor=(1.38, 1.03),ncol=1, fancybox=True)\n\n# normalized data\nax = fig.add_subplot(gs[1])\n\ncolors = ['blue', 'green', 'red', 'cyan', \n          'magenta', 'yellow', 'black', \n          'pink', 'lightgreen', 'lightblue', \n          'gray', 'indigo', 'orange']\n\nweights, params = [], []\nfor c in range(-4, 6):\n    lr_L2 = LogisticRegression(penalty='l2', C=10.**c, solver='liblinear', \n                            multi_class='ovr', random_state=0)\n    print(10.**c)\n    lr_L2.fit(X_train_std, y_train)\n    weights.append(lr_L2.coef_[1])\n    params.append(10**c)\n\nweights = np.array(weights)\n\nfor column, color in zip(range(weights.shape[1]), colors):\n    ax.plot(params, weights[:, column],\n             label=df_wine.columns[column + 1],\n             color=color)\n\nplt.title(\"Fitted weights for different regularization strengths (L2 Regularization)\",\n         fontdict={'weight':'normal','size': 14})\nplt.axhline(0, color='black', linestyle='--', linewidth=3)\nplt.axvline(x=0.007, color='gray', linestyle='--', label='C=0.007')\nplt.axvline(x=0.1, color='gray', linestyle=':', label='C=0.1')\nplt.xlim([10**(-5), 10**5])\nplt.ylabel('weight coefficient')\nplt.xlabel('C')\nplt.xscale('log')\nplt.legend(loc='upper left')\nax.legend(loc='upper center', bbox_to_anchor=(1.38, 1.03),ncol=1, fancybox=True)\n\nplt.show()\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n# Sequential Feature Selection {-}\n\nAn alternative to L1 regularization and sparsity as a method of feature selection is **dimensionality reduction**. There are two main techniques of dimensionality reduction:\n\n1. **Feature Selection** - select a subset of the original features which are relevant to making forecasts\n2. **Feature Extraction** - derive information from existing features to construct a new feature subspace \n    - This is done by compressing features in to a new, smaller set of features (will study this next week)\n\n**Feature selection problem:** Out of an initial set of $d$ features choose $k$ most relevant features where $k<d$.  \n\n- E.g. start with $d=100$ features and reduce them to $k=25$ most relevant features.\n- Search is typically done by using **sequential** feature selection algorithms\n- Sequential selection algorithms are a type of greedy search algorithm\n\n- **Greedy Search Algorithms** - make **locally optimal** choices at each stage of a combinatorial search problem but generally yield a **globally sub-optimal** solution\n    - In optimization, the terms \"locally optimal\" and \"globally optimal\" refer to the best solutions **within a specific context** or **the entire set of possibilities**, respectively.\n    - **Locally Optimal**: A solution is considered locally optimal if it is the best solution within a small surrounding neighborhood. It cannot be improved by making only small changes. However, it is not necessarily the best possible solution across the entire domain of solutions. In complex problems with many variables or in non-linear optimization, there can be many locally optimal solutions.\n    - **Globally Optimal**: A solution is globally optimal if it is the best solution across the entire domain of possible solutions. No other feasible solution provides a better outcome. Achieving a globally optimal solution guarantees that there is no other solution that could improve the result, regardless of how significant the change or where in the solution space it occurs.\n\n<br>\n\n- In the graph below, a greedy algorithm is trying to maximize the sum of values inside the circles\n    - To do this, it selects the largest number at each step of the algorithm   \n    - With a quick visual inspection of the graph, it is clear that this algorithm will not arrive at the correct solution   \n    - What is the correct solution?     \n    \n<img src=\"images/image8.gif\" alt=\"Drawing\" style=\"width: 450px;\"/>\n    \n<!-- ![](images/image8.jpg) -->",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "- In contrast **Exhaustive Search Algorithms** - evaluate all possible combinations and are guaranteed to find the optimal solution. However, an exhaustive search is often not computationally feasible.\n    - The correct solution for the largest sum is 7,3,1,99.\n    - This is clear to us because we can see that no other combination of nodes will come close to a sum of 110  \n    - The greedy algorithm fails to solve this problem because it makes decisions purely based on what the best answer at the time is: at each step it did choose the largest number   \n\n\n**Sequential Backward Selection (SBS)** is a type of sequential feature selection method\n- Start by including all possible $d$ features \n- Sequentially remove features one at a time\n- In order to determine which feature to remove at each stage define the criterion function $J$, such as the accuracy of classification\n- At each step remove a features which results in the least performance loss after its removal\n\n\n**SBS algorithm**\n1. Initialize the algorithm with $k=d$\n2. Determine the feature, $x^-$ that maximizes the criterion $x^-=\\underset{x}{\\text{argmax}}J(X_k-x)$ where $x\\in X_d$\n    - Note that $J(X_k-x)<J(X_k)$ meaning that the subset $X_k-x$ has lower accuracy that $X_k$\n3. Remove the feature $x^-$ from the feature set $X_k$ resulting in $X_{k-1}=X_k-x$\n    - E.g. $J$ = accuracy, $J(x_1, x_2, x_3) = 60\\%$, $J(x_1, x_2) = 50\\%$, $J(x_1, x_3)=40\\%$, $J(x_2, x_3)=30\\%$ -> choose $J(x_1, x_2) = 50\\%$, meaning we eliminated $x_3$ in this step\n4. Terminate if $k$ is equal to the number of desired features, otherwise go to step 2.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n### SBS Algorithm {-}\n\n- First, we'll try the SBS algorithm that comes with the textbook\n\n```\nfrom sklearn.base import clone\nfrom itertools import combinations\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\nclass SBS():\n    def __init__(self, estimator, k_features, scoring=accuracy_score,\n                 test_size=0.25, random_state=1):\n        self.scoring = scoring\n        self.estimator = clone(estimator)\n        self.k_features = k_features\n        self.test_size = test_size\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \n        X_train, X_test, y_train, y_test = \\\n            train_test_split(X, y, test_size=self.test_size,\n                             random_state=self.random_state, stratify=y)\n\n        stdsc = StandardScaler()\n        X_train = stdsc.fit_transform(X_train)\n        X_test = stdsc.transform(X_test)\n        \n        dim = X_train.shape[1]\n        self.indices_ = tuple(range(dim))\n        self.subsets_ = [self.indices_]\n        score = self._calc_score(X_train, y_train, \n                                 X_test, y_test, self.indices_)\n        self.scores_ = [score]\n\n        while dim > self.k_features:\n            scores = []\n            subsets = []\n\n            for p in combinations(self.indices_, r=dim - 1):\n                score = self._calc_score(X_train, y_train, \n                                         X_test, y_test, p)\n                scores.append(score)\n                subsets.append(p)\n\n            best = np.argmax(scores)\n            self.indices_ = subsets[best]\n            self.subsets_.append(self.indices_)\n            dim -= 1\n\n            self.scores_.append(scores[best])\n        self.k_score_ = self.scores_[-1]\n\n        return self\n\n    def transform(self, X):\n        return X[:, self.indices_]\n\n    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n        self.estimator.fit(X_train[:, indices], y_train)\n        y_pred = self.estimator.predict(X_test[:, indices])\n        score = self.scoring(y_test, y_pred)\n        # score = self.scoring(y_train, y_pred)\n        \n        return score\n\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n### Feature Selection with SBS algorithm and KNN {-}\n\nWe'll use the above SBS algorithm to select a subset of predictors out of 13 features\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n\n- Fit the SBS algorithm with KNN on wine training (standarised) data with `k_features = 1`  \n- Print optimal subsets starting from 1 to 13 features\n\n\n```\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# selecting features\nsbs = SBS(knn, k_features=1)\n\nsbs.fit(X, y)\n\nsbs.subsets_\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n- Plot the classification accuracy of the KNN classifier for each subset of features\n\n```\nk_feat = [len(k) for k in sbs.subsets_]\n\nprint(k_feat)\nprint(sbs.scores_)\n\nplt.plot(k_feat, sbs.scores_, marker='o')\nplt.ylim([0.7, 1.02])\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.grid()\nplt.tight_layout()\n# plt.savefig('images/sbs_knn', dpi=300)\nplt.show()\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n- Print the smallest feature subset with the classification accuracy of 100%  \n\n```\ndf_scores = pd.DataFrame(sbs.scores_, columns = ['Scores'])\ndf_scores['Feature Subsets'] = sbs.subsets_\nprint(df_scores)\n\nsmallest_100_subset = list(df_scores['Feature Subsets'].loc[7])\n\nprint(smallest_100_subset)\n\nprint('features', df_wine.columns[1:]) # start from 1 since 0 is class label\n\nprint(df_wine.columns[1:][smallest_100_subset])\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### scikit-learn has a Sequential Feature Selection libarary\n\n- Implemented somewhat differently (uses cross-validation which we will cover later in the course)\n- Does both `forward` and `backward` feature selection\n- Doesn't provide as much information as we have above\n- [https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) \n\n\n```\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\nknn2 = KNeighborsClassifier(n_neighbors=5)\nsfs = SequentialFeatureSelector(knn2, n_features_to_select=1, direction='backward')\nsfs.fit(X, y)\n\nsfs.get_support()\nnp.arange(X.shape[1])[sfs.get_support()]\n\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n---\n\n# Assessing Feature Importance with Random Forests {-}    \n\nUsing a random forest we can measure **feature importance** as the **averaged impurity decrease** computed from all decision trees in the forest\n- In scikit-learn access feature importance via \n    - `feature_importances` after fitting `RandomForestClassifier`\n    - `SelectFromModel` selects features based on a user-specified importance threshold\n\nA potential problem with feature importance:\n- If there is multicollinearity, i.e. features are highly correlated, it is difficult to disentangle importance\n    - One feature may rank very high while the other correlated features low (incorrectly)\n- Can check correlations between the features to see if multicollinearity is a problem in our application\n- If only interested in predictive ability then this is not a problem as we are not trying to interpret our results\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n<span style='background:orange'>  **Exercise 2: Understand the code below** \n    \na) Fit a random forest classifier to the wine data\n    \n- Note: Random Forest doesn't need data to be standardized   \n    \nb) Print feature importances  \nc) Plot feature importances  \nd) Select features with importance greater than 10%  \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "  \n\n- Fit a random forest classifier to the wine data (note: Random Forest doesn't need data to be standardized)\n\n\n```\nfrom sklearn.ensemble import RandomForestClassifier\n\nfeat_labels = df_wine.columns[1:]\n\nforest = RandomForestClassifier(n_estimators=500, random_state=1)\n\nforest.fit(X_train, y_train)\n```\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n- Print feature importances \n\n```\nimportances = forest.feature_importances_\n\n# print(importances)  # prints importances of each feature\n# print(np.argsort(importances)) # sort the index of importances from smallest to largest\n# print(np.argsort(importances)[::-1]) # reverses the index to get largest to smallest\n\n\nindices = np.argsort(importances)[::-1]\n\n# print(X_train.shape)\n\nfor f in range(X_train.shape[1]):\n#     print(f, indices[f])   # pick up positions from indices \n    print(f'{f+1:2})  {feat_labels[indices[f]]:40} {importances[indices[f]]:10}')\n\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n \n- Plot feature importances  \n\n```\nplt.title('Feature Importance')\nplt.bar(range(X_train.shape[1]), \n        importances[indices],\n        align='center')\n\nplt.xticks(range(X_train.shape[1]), \n           feat_labels[indices], rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.tight_layout()\n#plt.savefig('images/04_09.png', dpi=300)\nplt.show()    \n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<hr style=\"width:35%;margin-left:0;\">   \n\n- Select features with importance greater than 10% \n\n\n```\nfrom sklearn.feature_selection import SelectFromModel\n\nsfm = SelectFromModel(forest, threshold=0.1, prefit=True)\nX_selected = sfm.transform(X_train)\nprint('Number of features that meet this threshold criterion:', X_selected.shape[1])\n\nfor f in range(X_selected.shape[1]):\n    print(f'{f + 1:2}) {feat_labels[indices[f]]:40} {importances[indices[f]]:10}')\n    \n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}